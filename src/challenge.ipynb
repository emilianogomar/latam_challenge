{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from q1_memory import q1_memory\n",
    "import q1_memory\n",
    "import q1_time\n",
    "import q2_memory\n",
    "import q2_time\n",
    "import q3_memory\n",
    "import q3_time\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset modules to reload changes\n",
    "importlib.reload(q1_memory)\n",
    "importlib.reload(q1_time)\n",
    "importlib.reload(q2_memory)\n",
    "importlib.reload(q2_time)\n",
    "importlib.reload(q3_memory)\n",
    "importlib.reload(q3_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Path definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'C:/Users/emili/Documents/farmers-protest-tweets-2021-2-4.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if the file has duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the json file into a pandas DataFrame to check for duplicated rows\n",
    "df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "df = df.astype(str)\n",
    "\n",
    "df.duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_time.q1_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "\n",
    "cProfile.run('q2_time.q2_time(file_path)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "\n",
    "cProfile.run('q1_memory.q1_memory(file_path)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_memory.q1_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_time.q1_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2_memory.q2_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2_time.q2_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "\n",
    "df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "# Normalize the DataFrame\n",
    "normalized_user = json_normalize(df['user'])\n",
    "\n",
    "#Add json prefix to all json columns\n",
    "normalized_user.columns = [f'json_{col}' for col in normalized_user.columns]\n",
    "\n",
    "# Combine normalized DataFrame with original DataFrame\n",
    "df = pd.concat([df.drop(columns='user'), normalized_user], axis=1)\n",
    "\n",
    "# Drop all columns except Username and date\n",
    "columnas_a_mantener = ['date', 'json_username']\n",
    "\n",
    "# Eliminar todas las columnas excepto las dos especificadas\n",
    "df = df.drop(columns=[col for col in df.columns if col not in columnas_a_mantener])\n",
    "df['date'] = df['date'].dt.date\n",
    "df['json_username'] = df['json_username'].astype(str)\n",
    "\n",
    "df.info()\n",
    "\n",
    "top_fechas = df['date'].value_counts().head(10)\n",
    "\n",
    "top_fechas.head(10)\n",
    "\n",
    "usuarios_top_fechas = {}\n",
    "for fecha in top_fechas.index:\n",
    "    usuarios_top_fechas[fecha] = df[df['date'] == fecha]['json_username'].value_counts().idxmax()\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(\"Top 10 de fechas con más registros:\")\n",
    "print(top_fechas)\n",
    "print(\"\\nUsuario con más registros para cada fecha en el top 10:\")\n",
    "#for fecha, usuario in usuarios_top_fechas.items():\n",
    "#    print(f\"Fecha: {fecha}, Usuario: {usuario}\")\n",
    "\n",
    "#usuarios_top_fechas\n",
    "\n",
    "#lista_de_tuplas = list(df.to_records(index=False))\n",
    "\n",
    "# Mostrar la lista de tuplas resultante\n",
    "#lista_de_tuplas\n",
    "\n",
    "lista_de_tuplas = [(fecha, valor) for fecha, valor in usuarios_top_fechas.items()]\n",
    "\n",
    "lista_de_tuplas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "\n",
    "df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "# Normalize the DataFrame\n",
    "normalized_user = json_normalize(df['user'])\n",
    "\n",
    "#Add json prefix to all json columns\n",
    "normalized_user.columns = [f'json_{col}' for col in json_normalize(df['user']).columns]\n",
    "\n",
    "# Combine normalized DataFrame with original DataFrame\n",
    "df = pd.concat([df.drop(columns='user'), normalized_user], axis=1)\n",
    "\n",
    "# Convertir la columna de fecha a tipo datetime si no está en ese formato\n",
    "df['date'] = df['date'].dt.date\n",
    "\n",
    "# Agrupar por fecha y usuario, y contar el número de tweets por usuario en cada día\n",
    "tweets_por_dia_usuario = df.groupby('date')['json_username'].agg(lambda x: x.value_counts().idxmax()).reset_index()\n",
    "\n",
    "# Obtener el top 10 de días con más tweets\n",
    "top_10_dias = df.groupby('date').size().reset_index(name='count').nlargest(10,'count')\n",
    "\n",
    "# Para cada día en el top 10, obtener el usuario con más tweets\n",
    "top_usuarios_por_dia = top_10_dias.merge(tweets_por_dia_usuario, on='date', how='left')\n",
    "\n",
    "return_list = list(zip(top_usuarios_por_dia['date'], top_usuarios_por_dia['json_username']))\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"Top 10 de días con más tweets:\")\n",
    "print(return_list)\n",
    "\n",
    "print(\"Test 3 --------------------------------------------\")\n",
    "print(top_usuarios_por_dia)\n",
    "print(\"Test 3 --------------------------------------------\")\n",
    "\n",
    "print(\"\\nUsuarios con más tweets por día:\")\n",
    "print(top_usuarios_por_dia[['date', 'json_username']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import regex\n",
    "import emoji\n",
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "from datetime import datetime\n",
    "from pandas import json_normalize\n",
    "\n",
    "df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "# Inicializar un contador vacío para almacenar el recuento total de emojis\n",
    "total_emojis = Counter()\n",
    "\n",
    "# Iterar sobre cada texto de tweet y contar emojis, actualizando el contador total\n",
    "for tweet_text in df['content']:\n",
    "    #emojis = regex.findall(r'\\X', tweet_text)\n",
    "    emojis = [c for c in tweet_text if c in emoji.UNICODE_EMOJI]\n",
    "    total_emojis.update(Counter(emojis))\n",
    "\n",
    "# Obtener los 10 emojis más utilizados con sus respectivos conteos\n",
    "top_10_emojis = total_emojis.most_common(10)\n",
    "\n",
    "# Imprimir los 10 emojis más utilizados con sus conteos\n",
    "for emojis, count in top_10_emojis:\n",
    "    print(f\"Emoji: {emojis} - Cantidad: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import pandas as pd\n",
    "import regex\n",
    "from collections import Counter\n",
    "import gc\n",
    "import re\n",
    "\n",
    "def contains_emoji(text):\n",
    "    return bool(re.findall(r'[\\U0001F300-\\U0001F5FF\\u2600-\\u26FF\\u2700-\\u27BF]', text))\n",
    "\n",
    "\n",
    "df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "#df_content = df['content']\n",
    "\n",
    "emojis_df = df[df['content'].apply(contains_emoji)]\n",
    "\n",
    "# Inicializar un contador vacío para almacenar el recuento total de emojis\n",
    "total_emojis = Counter()\n",
    "\n",
    "# Iterar sobre cada texto de tweet y contar emojis, actualizando el contador total\n",
    "for tweet_text in df['content']:\n",
    "    emojis = regex.findall(r'[\\U0001F300-\\U0001F5FF\\U0001F600-\\U0001F64F\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002702-\\U000027B0\\U000024C2-\\U0001F251\\U0001f926-\\U0001f937\\U0001F1E0-\\U0001F1FF]{1}', tweet_text)\n",
    "    total_emojis.update(Counter(emojis))\n",
    "\n",
    "del df\n",
    "gc.collect()\n",
    "\n",
    "if '' in total_emojis:\n",
    "    print(\"encuentro uno\")\n",
    "\n",
    "del total_emojis['']\n",
    "\n",
    "# Obtener los 10 emojis más utilizados con sus respectivos conteos\n",
    "top_10_emojis = total_emojis.most_common(10)\n",
    "\n",
    "# Imprimir los 10 emojis más utilizados con sus conteos\n",
    "#for emoji, count in top_10_emojis:\n",
    "#    print(f\"Emoji: {emoji} - Cantidad: {count}\")\n",
    "\n",
    "emoji_list = [(emoji, count) for emoji, count in top_10_emojis]\n",
    "\n",
    "print(emoji_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "print(emoji.__version__)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
