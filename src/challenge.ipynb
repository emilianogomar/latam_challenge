{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from q1_memory import q1_memory\n",
    "import q1_memory\n",
    "import q1_time\n",
    "import importlib\n",
    "import memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'q1_time' from 'c:\\\\Users\\\\emili\\\\Documents\\\\latam_challenge\\\\src\\\\q1_time.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset modules to reload changes\n",
    "importlib.reload(q1_memory)\n",
    "importlib.reload(q1_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Path definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'C:/Users/emili/Documents/farmers-protest-tweets-2021-2-4.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if the file has duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the json file into a pandas DataFrame to check for duplicated rows\n",
    "df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "df = df.astype(str)\n",
    "\n",
    "df.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "\n",
    "# Define file path for the source twitter file\n",
    "file_path = r'C:/Users/emili/Documents/farmers-protest-tweets-2021-2-4.json'\n",
    "\n",
    "# Read the json file into a pandas DataFrame\n",
    "df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "# Normalize the DataFrame\n",
    "normalized_user = json_normalize(df['user'])\n",
    "\n",
    "#Add json prefix to all json columns\n",
    "normalized_user.columns = [f'json_{col}' for col in normalized_user.columns]\n",
    "\n",
    "# Combine normalized DataFrame with original DataFrame\n",
    "df = pd.concat([df.drop(columns='user'), normalized_user], axis=1)\n",
    "df = df.drop(columns='mentionedUsers')\n",
    "\n",
    "df = df.astype(str)\n",
    "\n",
    "df.head(10)\n",
    "\n",
    "df.duplicated().any()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: c:\\Users\\emili\\Documents\\latam_challenge\\src\\q1_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     7    125.5 MiB    125.5 MiB           1   @profile\n",
      "     8                                         def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "     9                                         \n",
      "    10                                             # read Json File\n",
      "    11   1417.7 MiB   1292.2 MiB           1       df = pd.read_json(file_path, lines=True)\n",
      "    12                                         \n",
      "    13                                             # Normalize user field\n",
      "    14   1421.8 MiB      4.1 MiB           1       normalized_user = json_normalize(df['user'])\n",
      "    15                                         \n",
      "    16                                             #Add json prefix to all json columns\n",
      "    17   1421.8 MiB      0.0 MiB          22       normalized_user.columns = [f'json_{col}' for col in normalized_user.columns]\n",
      "    18                                         \n",
      "    19                                             # Combine normalized DataFrame with original DataFrame\n",
      "    20   1464.4 MiB     42.6 MiB           1       df_2 = pd.concat([df.drop(columns='user'), normalized_user], axis=1)\n",
      "    21                                         \n",
      "    22                                             # Get top 10 dates with more tweets\n",
      "    23   1465.4 MiB      1.0 MiB           1       top_dates = df_2['date'].value_counts().head(10)\n",
      "    24                                         \n",
      "    25                                             # Get users with more tweets for top 10 dates \n",
      "    26   1465.4 MiB      0.0 MiB           1       users_top_dates = {}\n",
      "    27   1465.7 MiB      0.0 MiB          11       for fecha in top_dates.index:\n",
      "    28   1465.7 MiB      0.2 MiB          10           users_top_dates[fecha] = df_2[df_2['date'] == fecha]['json_username'].value_counts().idxmax()\n",
      "    29                                         \n",
      "    30                                             # Convert to proper output format\n",
      "    31   1465.7 MiB      0.0 MiB          11       date_list = [(fecha, valor) for fecha, valor in users_top_dates.items()]\n",
      "    32                                         \n",
      "    33   1465.7 MiB      0.0 MiB           1       return date_list\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(Timestamp('2021-02-20 12:57:17+0000', tz='UTC'), 'SivaKum66642898'),\n",
       " (Timestamp('2021-02-17 04:46:58+0000', tz='UTC'), 'RaaJVinderkaur'),\n",
       " (Timestamp('2021-02-15 02:43:08+0000', tz='UTC'), 'ajityadavdu'),\n",
       " (Timestamp('2021-02-23 05:05:55+0000', tz='UTC'), 'Cuttack_IYC'),\n",
       " (Timestamp('2021-02-17 01:12:32+0000', tz='UTC'), 'ImTrilokSingh'),\n",
       " (Timestamp('2021-02-17 02:42:18+0000', tz='UTC'), 'ajityadavdu'),\n",
       " (Timestamp('2021-02-12 04:52:08+0000', tz='UTC'), 'ajityadavdu'),\n",
       " (Timestamp('2021-02-19 22:15:55+0000', tz='UTC'), 'aman12b'),\n",
       " (Timestamp('2021-02-17 03:26:36+0000', tz='UTC'), 'Monica_Gill1'),\n",
       " (Timestamp('2021-02-14 14:34:48+0000', tz='UTC'), 'harpreet_067')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_time.q1_time(file_path)\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: py-spy in c:\\users\\emili\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.3.14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: C:\\Users\\emili\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install py-spy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: c:\\Users\\emili\\Documents\\latam_challenge\\src\\q1_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     8    191.8 MiB    191.8 MiB           1   @profile\n",
      "     9                                         def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    10                                             \n",
      "    11                                             # read Json File\n",
      "    12   1402.9 MiB   1211.1 MiB           1       df = pd.read_json(file_path, lines=True)\n",
      "    13                                         \n",
      "    14                                             # Normalize user field\n",
      "    15   1427.0 MiB     24.1 MiB           1       normalized_user = json_normalize(df['user'])\n",
      "    16                                         \n",
      "    17                                             #Add json prefix to all json columns\n",
      "    18   1427.0 MiB      0.0 MiB          22       normalized_user.columns = [f'json_{col}' for col in normalized_user.columns]\n",
      "    19                                         \n",
      "    20                                             # Combine normalized DataFrame with original DataFrame and delete the normalized DF\n",
      "    21   1444.9 MiB     17.9 MiB           1       df = pd.concat([df.drop(columns='user'), normalized_user], axis=1)\n",
      "    22   1426.8 MiB    -18.1 MiB           1       del normalized_user\n",
      "    23                                         \n",
      "    24                                             # Drop all columns except username and date\n",
      "    25   1426.8 MiB      0.0 MiB           1       columnas_a_mantener = ['date', 'json_username']\n",
      "    26   1426.8 MiB    -95.2 MiB          42       df = df.drop(columns=[col for col in df.columns if col not in columnas_a_mantener])\n",
      "    27                                         \n",
      "    28                                             # Change columns types\n",
      "    29   1332.6 MiB    -94.2 MiB           1       df['date'] = df['date'].dt.date\n",
      "    30   1332.6 MiB      0.0 MiB           1       df['json_username'] = df['json_username'].astype(str)\n",
      "    31                                         \n",
      "    32                                             # Release non use memory\n",
      "    33   1331.7 MiB     -0.9 MiB           1       gc.collect()\n",
      "    34                                         \n",
      "    35                                             # Get top 10 dates with more tweets\n",
      "    36   1331.4 MiB     -0.2 MiB           1       top_dates = df['date'].value_counts().head(10)\n",
      "    37                                         \n",
      "    38                                             # Get users with more tweets for top 10 dates \n",
      "    39   1331.4 MiB      0.0 MiB           1       users_top_dates = {}\n",
      "    40   1331.5 MiB     -0.3 MiB          11       for fecha in top_dates.index:\n",
      "    41   1331.5 MiB     -0.3 MiB          10           users_top_dates[fecha] = df[df['date'] == fecha]['json_username'].value_counts().idxmax()\n",
      "    42                                         \n",
      "    43    206.8 MiB  -1124.7 MiB           1       del df\n",
      "    44    206.8 MiB      0.0 MiB           1       gc.collect()\n",
      "    45                                         \n",
      "    46                                             # Convert to proper output format\n",
      "    47    206.8 MiB      0.0 MiB          11       date_list = [(fecha, valor) for fecha, valor in users_top_dates.items()]\n",
      "    48                                         \n",
      "    49    206.8 MiB      0.0 MiB           1       return date_list\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
       " (datetime.date(2021, 2, 19), 'Preetm91')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_memory.q1_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "\n",
    "df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "# Normalize the DataFrame\n",
    "normalized_user = json_normalize(df['user'])\n",
    "\n",
    "#Add json prefix to all json columns\n",
    "normalized_user.columns = [f'json_{col}' for col in normalized_user.columns]\n",
    "\n",
    "# Combine normalized DataFrame with original DataFrame\n",
    "df = pd.concat([df.drop(columns='user'), normalized_user], axis=1)\n",
    "\n",
    "# Drop all columns except Username and date\n",
    "columnas_a_mantener = ['date', 'json_username']\n",
    "\n",
    "# Eliminar todas las columnas excepto las dos especificadas\n",
    "df = df.drop(columns=[col for col in df.columns if col not in columnas_a_mantener])\n",
    "df['date'] = df['date'].dt.date\n",
    "df['json_username'] = df['json_username'].astype(str)\n",
    "\n",
    "df.info()\n",
    "\n",
    "top_fechas = df['date'].value_counts().head(10)\n",
    "\n",
    "top_fechas.head(10)\n",
    "\n",
    "usuarios_top_fechas = {}\n",
    "for fecha in top_fechas.index:\n",
    "    usuarios_top_fechas[fecha] = df[df['date'] == fecha]['json_username'].value_counts().idxmax()\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(\"Top 10 de fechas con más registros:\")\n",
    "print(top_fechas)\n",
    "print(\"\\nUsuario con más registros para cada fecha en el top 10:\")\n",
    "#for fecha, usuario in usuarios_top_fechas.items():\n",
    "#    print(f\"Fecha: {fecha}, Usuario: {usuario}\")\n",
    "\n",
    "#usuarios_top_fechas\n",
    "\n",
    "#lista_de_tuplas = list(df.to_records(index=False))\n",
    "\n",
    "# Mostrar la lista de tuplas resultante\n",
    "#lista_de_tuplas\n",
    "\n",
    "lista_de_tuplas = [(fecha, valor) for fecha, valor in usuarios_top_fechas.items()]\n",
    "\n",
    "lista_de_tuplas\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
